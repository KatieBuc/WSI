{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46976620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.prepared import prep\n",
    "from shapely.geometry import Point\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from wsi.mapping.iso_name import ISO_NAME\n",
    "from wsi.mapping.iso_gw import ISO_GW\n",
    "from wsi.mapping.iso_iso2 import ISO_ISO2\n",
    "from wsi.utils import raw_data_path, processed_data_path\n",
    "\n",
    "# Constants\n",
    "EARTH_RADIUS_KM = 6371\n",
    "FILE_PATTERN = \"gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_30_sec_{tile}.asc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225e09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename=processed_data_path(\"shocks\",\"proximity_conflict\", 'conflict_logs.log'),   # Output file path\n",
    "    filemode='a',                          # Append mode\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d018aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_population_count(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    with open(file_path, 'r') as f:\n",
    "        metadata = {}\n",
    "        for _ in range(6):\n",
    "            key, value = f.readline().strip().split()\n",
    "            metadata[key.lower()] = float(value)\n",
    "    data = np.loadtxt(file_path, skiprows=6)\n",
    "    gt = (\n",
    "        metadata['xllcorner'],\n",
    "        metadata['cellsize'],\n",
    "        0,\n",
    "        metadata['yllcorner'] + metadata['nrows'] * metadata['cellsize'],\n",
    "        0,\n",
    "        -metadata['cellsize']\n",
    "    )\n",
    "    return {\n",
    "        \"file\": file_path,\n",
    "        \"data\": data,\n",
    "        \"geotransform\": gt,\n",
    "        \"no_data_value\": metadata['nodata_value']\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_pixel_grid(geotransform, shape):\n",
    "    origin_x, pixel_w, _, origin_y, _, pixel_h = geotransform\n",
    "    rows, cols = shape\n",
    "    row_grid, col_grid = np.ogrid[0:rows, 0:cols]\n",
    "    lat_grid = origin_y + row_grid * pixel_h\n",
    "    lon_grid = origin_x + col_grid * pixel_w\n",
    "    lat_grid = np.broadcast_to(lat_grid, (rows, cols))\n",
    "    lon_grid = np.broadcast_to(lon_grid, (rows, cols))\n",
    "    return lat_grid, lon_grid\n",
    "\n",
    "def haversine_distance_vector(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1 = math.radians(lat1), math.radians(lon1)\n",
    "    lat2, lon2 = np.radians(lat2), np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    return EARTH_RADIUS_KM * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "def get_population_in_conflict_area(all_data, conflict_coords, radius_km=50):\n",
    "    total_population = 0\n",
    "    union_grid_points = []\n",
    "    radius_deg_lat = radius_km / 111.0\n",
    "\n",
    "    for dataset in all_data:\n",
    "        data = dataset[\"data\"]\n",
    "        gt = dataset[\"geotransform\"]\n",
    "        nodata = dataset[\"no_data_value\"]\n",
    "        lat_grid, lon_grid = prepare_pixel_grid(gt, data.shape)\n",
    "        mask = np.zeros(data.shape, dtype=bool)\n",
    "\n",
    "        for lat, lon in conflict_coords:\n",
    "            radius_deg_lon = radius_km / (111.0 * math.cos(math.radians(lat)))\n",
    "            lat_min, lat_max = lat - radius_deg_lat, lat + radius_deg_lat\n",
    "            lon_min, lon_max = lon - radius_deg_lon, lon + radius_deg_lon\n",
    "            conflict_mask = (\n",
    "                (lat_grid >= lat_min) & (lat_grid <= lat_max) &\n",
    "                (lon_grid >= lon_min) & (lon_grid <= lon_max)\n",
    "            )\n",
    "            dists = haversine_distance_vector(lat, lon, lat_grid[conflict_mask], lon_grid[conflict_mask])\n",
    "            tmp_mask = np.zeros_like(mask)\n",
    "            tmp_mask[conflict_mask] = dists <= radius_km\n",
    "            mask |= tmp_mask\n",
    "\n",
    "        valid_mask = mask & (data != nodata)\n",
    "        total_population += data[valid_mask].sum()\n",
    "        if np.any(valid_mask):\n",
    "            union_grid_points += np.column_stack((lat_grid[valid_mask], lon_grid[valid_mask], data[valid_mask])).tolist()\n",
    "\n",
    "    return total_population, union_grid_points\n",
    "\n",
    "def clip_grid_points_to_country(grid_points, country_polygon):\n",
    "    prepped = prep(country_polygon)\n",
    "    return [pt for pt in grid_points if prepped.contains(Point(pt[1], pt[0]))]\n",
    "\n",
    "def filter_conflicts(df, country_code, year):\n",
    "    return df[(df['year'] == year) & df['country_id'].astype(str).str.contains(str(country_code))]\n",
    "\n",
    "def get_conflict_coordinates(df):\n",
    "    return df[['latitude', 'longitude']].dropna().values.tolist()\n",
    "\n",
    "def process_country_code(country_code, years, countries, event_csv, df_pop, all_data):\n",
    "    summary_rows = []\n",
    "    heatmap_points = []\n",
    "\n",
    "    iso3 = next((iso for iso, code in ISO_GW.items() if str(code) == country_code), None)\n",
    "    if not iso3:\n",
    "        logger.warning(f\"ISO3 code not found for country_code: {country_code}\")\n",
    "        return None\n",
    "\n",
    "    iso2 = ISO_ISO2[iso3]\n",
    "    country_gdf = countries[countries['ISO'] == iso2]\n",
    "    if country_gdf.empty:\n",
    "        logger.warning(f\"Country geometry not found for ISO3: {iso3}/ ISO2: {iso2})\")\n",
    "        return None\n",
    "\n",
    "    polygon = country_gdf.geometry.iloc[0]\n",
    "\n",
    "    for yr in years:\n",
    "        conflict_df = filter_conflicts(event_csv, country_code, yr)\n",
    "        coords = get_conflict_coordinates(conflict_df)\n",
    "\n",
    "        if not coords:\n",
    "            pop_in_conflict = 0\n",
    "            union_grid_points = []\n",
    "        else:\n",
    "            pop_in_conflict, union_grid_points = get_population_in_conflict_area(all_data, coords)\n",
    "            union_grid_points = clip_grid_points_to_country(union_grid_points, polygon)\n",
    "            pop_in_conflict = sum(pt[2] for pt in union_grid_points)\n",
    "            # store grid with year tag\n",
    "            # for pt in union_grid_points:\n",
    "            #     heatmap_points.append({\n",
    "            #         'year': yr,\n",
    "            #         'latitude': pt[0],\n",
    "            #         'longitude': pt[1],\n",
    "            #         'population': pt[2]\n",
    "            #     })\n",
    "\n",
    "        national_pop = df_pop[(df_pop['ISO_code'] == iso3) & (df_pop['Year'] == yr)]['Population']\n",
    "        if not national_pop.empty and national_pop.iloc[0] > 0:\n",
    "            pct = (pop_in_conflict / national_pop.iloc[0]) * 100\n",
    "        else:\n",
    "            pct = None\n",
    "\n",
    "        summary_rows.append({\n",
    "            'gw_code': country_code,\n",
    "            'iso3': iso3,\n",
    "            'year': yr,\n",
    "            'pop_in_conflict': pop_in_conflict,\n",
    "            'national_pop': national_pop.iloc[0] if not national_pop.empty else None,\n",
    "            'percent': pct\n",
    "        })\n",
    "\n",
    "    # Save individual files\n",
    "    pd.DataFrame(summary_rows).to_csv(processed_data_path(\"shocks\", \"proximity_conflict\", f\"conflict_summary_{iso3}.csv\"),index=False)\n",
    "    # pd.DataFrame(heatmap_points).to_csv(processed_data_path(\"shocks\", \"proximity_conflict\", f\"heatmap_grid_{iso3}.csv\"),index=False)\n",
    "\n",
    "    return iso3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff82df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbuc0011\\AppData\\Local\\Temp\\ipykernel_1788\\1222629374.py:17: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  event_csv = pd.read_csv(raw_data_path(\"shocks\", \"GEDEvent_v25_1.csv\"))\n"
     ]
    }
   ],
   "source": [
    "# Load shared data (outside parallel scope)\n",
    "\n",
    "## POPULATION DNESITY\n",
    "all_data = []\n",
    "for tile in range(1, 9):\n",
    "    fp = raw_data_path(\"shocks\", \"gpw-v4\", FILE_PATTERN.format(tile=tile))\n",
    "    result = read_population_count(fp)\n",
    "    if result:\n",
    "        all_data.append(result)\n",
    "\n",
    "## SHAPEFILE\n",
    "# TODO: make secondary shapefile dataset when country not availbale in first\n",
    "countries = gpd.read_file(raw_data_path(\"shocks\", \"country_shapefiles\", \"World_Countries_Generalized.shp\")).to_crs(\"EPSG:4326\")\n",
    "\n",
    "## CONFLICT EVENTS\n",
    "UcdpPrioConflict_csv = pd.read_csv(raw_data_path(\"shocks\", \"UcdpPrioConflict_v25_1.csv\"))\n",
    "event_csv = pd.read_csv(raw_data_path(\"shocks\", \"GEDEvent_v25_1.csv\"))\n",
    "event_csv = event_csv[event_csv['conflict_new_id'].isin(UcdpPrioConflict_csv['conflict_id'].unique())]\n",
    "\n",
    "# fitler events, at least one fatality, also more than one death at event per country per year per dyad (i.e. exclude small conflicts)\n",
    "event_csv = event_csv[event_csv['best'] > 0]\n",
    "\n",
    "# total deaths per dyad-country-year\n",
    "death_sums = (\n",
    "    event_csv.groupby(['dyad_new_id', 'country_id', 'year'])['best']\n",
    "    .sum()\n",
    "    .reset_index(name='group_best_sum')\n",
    ")\n",
    "\n",
    "# Keep only groups where total deaths > 1\n",
    "valid_groups = death_sums[death_sums['group_best_sum'] > 1]\n",
    "\n",
    "# Merge back to filter the original event-level data\n",
    "event_csv = event_csv.merge(\n",
    "    valid_groups[['dyad_new_id', 'country_id', 'year']],\n",
    "    on=['dyad_new_id', 'country_id', 'year'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "## TOTAL POPULATION\n",
    "from wsi.shocks.population import build_population_df\n",
    "df_pop = build_population_df()\n",
    "\n",
    "# Save all lat/long of relevant events\n",
    "# Invert ISO_GW: {GW_code → ISO3}\n",
    "GW_ISO = {str(v): k for k, v in ISO_GW.items()}\n",
    "event_csv['ISO3'] = event_csv['country_id'].astype(str).map(GW_ISO)\n",
    "all_events = event_csv[['year', 'country_id', 'conflict_name', 'dyad_name', 'best','latitude', 'longitude']].copy()\n",
    "all_events.to_csv(processed_data_path(\"shocks\", \"proximity_conflict\", f\"event_level_coords.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6825bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'AUS', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD', 'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BOL', 'BRA', 'BRB', 'BRN', 'BTN', 'BWA', 'CAN', 'CHL', 'CHN', 'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CRI', 'CUB', 'CYP', 'CZE', 'DEU', 'DJI', 'DMA', 'DNK', 'DOM', 'DZA', 'ECU', 'EGY', 'ERI', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM', 'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC', 'GRD', 'GTM', 'GUY', 'HND', 'HRV', 'HTI', 'HUN', 'IDN', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA', 'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KIR', 'KNA', 'KOR', 'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LIE', 'LKA', 'LSO', 'LTU', 'LUX', 'LVA', 'MAR', 'MCO', 'MDA', 'MDG', 'MDV', 'MHL', 'MLI', 'MLT', 'MMR', 'MNG', 'MOZ', 'MRT', 'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NIC', 'NLD', 'NOR', 'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PNG', 'POL', 'PRK', 'PRT', 'PRY', 'PSE', 'QAT', 'ROU', 'RUS', 'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SOM', 'SRB', 'SSD', 'STP', 'SUR', 'SVN', 'SWE', 'SWZ', 'SYC', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON', 'TUN', 'TUR', 'TUV', 'UGA', 'URY', 'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB', 'ZWE']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory = processed_data_path(\"shocks\", \"proximity_conflict\")\n",
    "\n",
    "# Pattern to match filenames like conflict_summary_ABC.csv\n",
    "pattern = re.compile(r\"conflict_summary_([A-Z]{3})\\.csv\")\n",
    "\n",
    "# List all files and extract matching ISO codes\n",
    "iso_codes = []\n",
    "for filename in os.listdir(directory):\n",
    "    match = pattern.match(filename)\n",
    "    if match:\n",
    "        iso_codes.append(match.group(1))\n",
    "\n",
    "# Sort and get unique ISO codes\n",
    "iso_codes = sorted(set(iso_codes))\n",
    "print(iso_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e8f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_to_gw = [ISO_GW[iso] for iso in iso_codes if iso in ISO_GW]\n",
    "valid_gw_codes = GW_ISO.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37185fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution\n",
    "years = list(range(1995,2025))\n",
    "\n",
    "valid_gw_codes = list(valid_gw_codes - set(iso_to_gw)) + [700] #Afghanistan\n",
    "#[\"811\", \"840\", \"850\", \"900\"]  # Cambodia, Phillipines, Indonesia, Australia\n",
    "#valid_gw_codes = [\"900\"] \n",
    "#valid_gw_codes = GW_ISO.keys()\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "completed = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_country_code, code, years, countries, event_csv, df_pop, all_data): code for code in valid_gw_codes}\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            completed.append(result)\n",
    "            print(f\"✅ Saved results for {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
